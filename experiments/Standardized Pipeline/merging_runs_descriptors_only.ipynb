{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_dataframe(path, extra_column=\"\", extra_column_value=\"\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        contents = f.readlines()\n",
    "    if contents[1][-2] == \"}\":\n",
    "        unfinished = True\n",
    "    else:\n",
    "        unfinished = False\n",
    "    if unfinished:\n",
    "        columns = contents[0].split(\",\")\n",
    "        columns[-1] = columns[-1][:-1]\n",
    "        data = {}\n",
    "        for index, key in enumerate(contents[0].split(\",\")):\n",
    "            if index == len(contents[0].split(\",\")) - 1:\n",
    "                key = key[:-1]\n",
    "            data[key] = []\n",
    "        for row in contents[1:]:\n",
    "            parameters = \"{\" + row.split(\"{\")[1][:-1]\n",
    "            data[\"hyperparams\"].append(parameters)\n",
    "\n",
    "            elements = row.split(\"{\")[0].split(\",\")[1:-1]\n",
    "            data[\"model\"].append(elements[-1])\n",
    "\n",
    "            for index, key in enumerate(list(data.keys())[2:]):\n",
    "                data[key].append(float(elements[index]))\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = pd.read_csv(path)\n",
    "        col_to_drop = []\n",
    "        for column in df.columns:\n",
    "            if len(column) > 7 and column[:7] == \"Unnamed\":\n",
    "                col_to_drop.append(column)\n",
    "        for col in col_to_drop:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    if extra_column != \"\":\n",
    "        df[extra_column] = extra_column_value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3360, 11)\n",
      "(407, 11) (3767, 11)\n",
      "(2656, 11) (6423, 11)\n",
      "(2460, 11) (8883, 11)\n",
      "(2456, 11) (11339, 11)\n",
      "(2735, 11) (14074, 11)\n",
      "(2493, 11) (16567, 11)\n",
      "(2404, 11) (18971, 11)\n",
      "(2404, 11) (21375, 11)\n",
      "Index(['model', 'hyperparams', 'mse', 'train_mse', 'rmse', 'train_rmse', 'mae',\n",
      "       'train_mae', 'r2', 'train_r2', 'split'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#file_paths.append(r\"C:\\Users\\wojci\\Pulpit\\inzynierka-asia\\01_split\\Asia_split0.1_bace_classification_run.csv\")\n",
    "#file_paths.append(r\"C:\\Users\\wojci\\Pulpit\\inzynierka-asia\\02_split\\Asia_split0.2_bace_classification_run.csv\")\n",
    "#file_paths.append(r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\Standardized Pipeline\\Merged runs\\Intermediate merges\\Descriptors_bace_classification_3.csv\")\n",
    "#out_path = r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\Standardized Pipeline\\Merged runs\\Descriptors_bace_classification_merged.csv\"\n",
    "#\n",
    "#extra_column = \"split\"\n",
    "##extra_column_values = [\"0.1\", \"0.2\"]\n",
    "#extra_column_values = [\"0.\" + str(i+1) for i in range(len(file_paths))]\n",
    "#print(extra_column_values)\n",
    "\n",
    "#file_paths.append(r\"C:\\Users\\wojci\\Pulpit\\inzynierka-asia\\03_split\\Asia_split0.3_ROR__classification_run_nogb.csv\")\n",
    "\n",
    "collect_splits = False\n",
    "if collect_splits:\n",
    "    for number in range(3, 10):\n",
    "        file_paths = []\n",
    "        folder = r\"C:\\Users\\wojci\\Pulpit\\inzynierka-asia\\0\" + str(number) + r\"_split\"\n",
    "        datasets = []\n",
    "\n",
    "        for (dirpath, dirnames, filenames) in os.walk(folder):\n",
    "            datasets.extend(filenames)\n",
    "            break\n",
    "\n",
    "        prefix = r\"Asia_split0.\" + str(number) + r\"_ROR__regre\" \n",
    "        for dataset in datasets:\n",
    "            if dataset[:len(prefix)] == prefix:\n",
    "                print(dataset)\n",
    "                file_paths.append(os.path.join(folder, dataset))\n",
    "\n",
    "        out_path = r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\Standardized Pipeline\\Merged runs\\Descriptors_ROR_regression_\" + str(number) + r\".csv\"\n",
    "\n",
    "        for i, file_path in enumerate(file_paths):\n",
    "            if i == 0:\n",
    "                df = get_correct_dataframe(file_path)\n",
    "                print(df.shape)\n",
    "            else:\n",
    "                df1 = get_correct_dataframe(file_path)\n",
    "                df = pd.concat([df, df1])\n",
    "                \n",
    "                df.drop_duplicates(subset=['hyperparams'], inplace=True, keep='last')\n",
    "                print(df1.shape, df.shape)\n",
    "        df.to_csv(out_path)\n",
    "\n",
    "else:\n",
    "    file_paths = []\n",
    "    folder = r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\Standardized Pipeline\\Merged runs\\Intermediate merges\\bace regre\"\n",
    "    datasets = []\n",
    "\n",
    "    file_paths.append(r\"C:\\Users\\wojci\\Pulpit\\inzynierka-asia\\01_split\\Asia_split0.1_bace_regression_run.csv\")\n",
    "    file_paths.append(r\"C:\\Users\\wojci\\Pulpit\\inzynierka-asia\\02_split\\Asia_split0.2_bace_regression_run.csv\")\n",
    "\n",
    "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
    "        datasets.extend(filenames)\n",
    "        break\n",
    "\n",
    "    #offset = len(\"Kuba_split0.x_\")\n",
    "    #prefix = r\"ROR__class\"\n",
    "    #for dataset in datasets:\n",
    "    #    if dataset[offset:offset+len(prefix)] == prefix:\n",
    "    #        print(dataset)\n",
    "    #        file_paths.append(os.path.join(folder, dataset))\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        file_paths.append(os.path.join(folder, dataset))\n",
    "\n",
    "\n",
    "    out_path = r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\Standardized Pipeline\\Merged runs\\Descriptors_BACE_regression_merged.csv\"\n",
    "\n",
    "    extra_column = \"split\"\n",
    "    #extra_column_values = [\"0.\" + str(i+1) for i in range(len(file_paths))]\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        #print(file_path)\n",
    "        \n",
    "        #print(file_path.split(\"\\\\\")[-1][10:13])\n",
    "        if i == 0:\n",
    "            if extra_column != \"\":\n",
    "                df = get_correct_dataframe(file_path, extra_column, file_path.split(\"\\\\\")[-1][10:13])\n",
    "            else:\n",
    "                df = get_correct_dataframe(file_path)\n",
    "            print(df.shape)\n",
    "        else:\n",
    "            if extra_column != \"\":\n",
    "                df1 = get_correct_dataframe(file_path, extra_column, file_path.split(\"\\\\\")[-1][10:13])\n",
    "            else:\n",
    "                df1 = get_correct_dataframe(file_path)\n",
    "            df = pd.concat([df, df1])\n",
    "            \n",
    "            print(df1.shape, df.shape)\n",
    "    df.to_csv(out_path)\n",
    "    \n",
    "    print(df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import deepchem as dc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_descriptors = True\n",
    "use_fingerprints = False\n",
    "\n",
    "regression = True\n",
    "\n",
    "models_with_parameters = [['rf', {\n",
    "  \"bootstrap\": True,\n",
    "  #\"criterion\": \"entropy\",\n",
    "  \"criterion\": \"squared_error\",\n",
    "  \"min_samples_split\": 32,\n",
    "  \"n_estimators\": 30\n",
    "}]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data the old way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input standard SMILES column\n",
    "def CalculateMorganFingerprint(mol):\n",
    "    mol = mol.apply(Chem.MolFromSmiles)\n",
    "    mfpgen = AllChem.GetMorganGenerator(radius=2,fpSize=2048)\n",
    "    fingerprint = np.array([mfpgen.GetFingerprintAsNumPy(x) for x in mol])\n",
    "    fingerprint = pd.DataFrame(fingerprint, columns = ['mfp'+str(i) for i in range(fingerprint.shape[1])])\n",
    "    return fingerprint\n",
    "\n",
    "### Input standard SMILES column\n",
    "def CalculateDescriptors(mol):\n",
    "    mol = mol.apply(Chem.MolFromSmiles)\n",
    "    calc = Calculator(descriptors, ignore_3D=False)\n",
    "    X_mordred = calc.pandas(mol, nproc=1)\n",
    "    X_mordred = X_mordred.select_dtypes(['number'])\n",
    "    #normalize\n",
    "    X_mordred = (X_mordred-X_mordred.min())/(X_mordred.max()-X_mordred.min())\n",
    "    #drop columns wth low std\n",
    "    X_mordred = X_mordred.loc[:,X_mordred.std()>0.01]\n",
    "    return X_mordred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_downloaded_CSV_BACE(path, regression = False):\n",
    "    df = pd.read_csv(path)\n",
    "    df.drop_duplicates('mol')\n",
    "    df = df.dropna()\n",
    "    #df.drop(['CID', 'canvasUID'], axis=1, inplace=True)\n",
    "\n",
    "    if regression:\n",
    "        df['Target'] = df['pIC50']\n",
    "        df.drop('Class', axis=1, inplace=True)\n",
    "        df.drop('pIC50', axis=1, inplace=True)\n",
    "    else:\n",
    "        df['Target'] = df['Class']\n",
    "        df.drop('Class', axis=1, inplace=True)\n",
    "        df.drop('pIC50', axis=1, inplace=True)\n",
    "\n",
    "    df = df[['mol', 'Target', 'Model']]\n",
    "\n",
    "    if use_descriptors:\n",
    "        new_df = CalculateDescriptors(df['mol'])\n",
    "    if use_fingerprints:\n",
    "        new_df = CalculateMorganFingerprint(df['mol'])\n",
    "        \n",
    "    new_df['Target'] = df['Target']\n",
    "    new_df['Model'] = df['Model']\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def Split_downloaded_CSV_BACE(df, scaffold=True):\n",
    "    if not scaffold:\n",
    "        X = df.drop(['Target', 'Model'], axis=1)\n",
    "        y = df[['Target']]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.111, random_state=42)\n",
    "        return X_train, y_train, X_test, y_test, X_valid, y_valid\n",
    "\n",
    "    X = df.drop(['Target'], axis=1)\n",
    "    y = df[['Target', 'Model']]\n",
    "\n",
    "    X_train = X[X['Model'] == 'Train']\n",
    "    y_train = y[y['Model'] == 'Train']\n",
    "    X_test = X[X['Model'] == 'Test']\n",
    "    y_test = y[y['Model'] == 'Test']\n",
    "    X_valid = X[X['Model'] == 'Valid']\n",
    "    y_valid = y[y['Model'] == 'Valid']\n",
    "    \n",
    "    X_train = X_train.drop('Model', axis=1)\n",
    "    y_train = y_train.drop('Model', axis=1)\n",
    "    X_test = X_test.drop('Model', axis=1)\n",
    "    y_test = y_test.drop('Model', axis=1)\n",
    "    X_valid = X_valid.drop('Model', axis=1)\n",
    "    y_valid = y_valid.drop('Model', axis=1)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wojci\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "100%|██████████| 1513/1513 [04:22<00:00,  5.76it/s]\n",
      "C:\\Users\\wojci\\AppData\\Local\\Temp\\ipykernel_9508\\898515923.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['Target'] = df['Target']\n",
      "C:\\Users\\wojci\\AppData\\Local\\Temp\\ipykernel_9508\\898515923.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df['Model'] = df['Model']\n"
     ]
    }
   ],
   "source": [
    "df_loaded = Load_downloaded_CSV_BACE(r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\BACE\\bace.csv\", regression=regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold_split = True\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_valid, y_valid = Split_downloaded_CSV_BACE(df_loaded, scaffold=scaffold_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203, 1222)\n",
      "(1265, 1222)\n",
      "(45, 1222)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(model_name, hyperparams):\n",
    "    if model_name == 'rf':\n",
    "        if regression:\n",
    "            model = RandomForestRegressor(n_estimators=hyperparams[\"n_estimators\"],\n",
    "                                    min_samples_split=hyperparams[\"min_samples_split\"],\n",
    "                                    criterion=hyperparams[\"criterion\"],\n",
    "                                    bootstrap=hyperparams[\"bootstrap\"])\n",
    "        else:\n",
    "            model = RandomForestClassifier(n_estimators=hyperparams[\"n_estimators\"],\n",
    "                                    min_samples_split=hyperparams[\"min_samples_split\"], \n",
    "                                    criterion=hyperparams[\"criterion\"],\n",
    "                                    bootstrap=hyperparams[\"bootstrap\"])\n",
    "            \n",
    "    #if model_name == '':\n",
    "    #    if regression:\n",
    "    #        model = RandomForestRegressor(n_estimators=hyperparams[\"n_estimators\"],\n",
    "    #                                min_samples_split=hyperparams[\"min_samples_split\"],\n",
    "    #                                criterion=hyperparams[\"criterion\"],\n",
    "    #                                bootstrap=hyperparams[\"bootstrap\"])\n",
    "    #    else:\n",
    "    #        model = RandomForestClassifier(n_estimators=hyperparams[\"n_estimators\"],\n",
    "    #                                min_samples_split=hyperparams[\"min_samples_split\"], \n",
    "    #                                criterion=hyperparams[\"criterion\"],\n",
    "    #                                bootstrap=hyperparams[\"bootstrap\"])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, X_train, y_train, X_test, y_test, X_valid, y_valid, regression, metrics=[], iterations=1):\n",
    "    for i in range(iterations):\n",
    "        model.fit(X_train, np.reshape(y_train, (-1, )))\n",
    "        \n",
    "        y_test_predicted = model.predict(X_test)\n",
    "        y_valid_predicted = model.predict(X_valid)\n",
    "\n",
    "        print(\"Standard train-test results:\")\n",
    "\n",
    "        if regression:\n",
    "            rmse_test = mean_squared_error(y_test, y_test_predicted, squared=False)\n",
    "            rmse_valid = mean_squared_error(y_valid, y_valid_predicted, squared=False)\n",
    "            print(rmse_test)\n",
    "            print(rmse_valid)\n",
    "            \n",
    "        else:\n",
    "            roc_auc_test = roc_auc_score(y_test, y_test_predicted)\n",
    "            roc_auc_valid = roc_auc_score(y_valid, y_valid_predicted)\n",
    "\n",
    "            print(roc_auc_test)\n",
    "            print(roc_auc_valid)\n",
    "\n",
    "def benchmark_train_and_test(model, X_train, y_train, X_test, y_test, X_valid, y_valid, regression, metrics=[], iterations=1):\n",
    "    model = dc.models.SklearnModel(model) ### for benchmark \n",
    "    for i in range(iterations):\n",
    "        #print(X_train.to_numpy().shape)\n",
    "        #print(y_train.to_numpy().shape)\n",
    "        #print(np.ones_like(y_train).shape)\n",
    "        #print(np.array([i for i in range(y_train.shape[0])]).shape)\n",
    "\n",
    "        train_set = dc.data.DiskDataset.from_numpy(X=X_train.to_numpy(), y=y_train.to_numpy(), w=np.ones_like(y_train), ids=np.array([i for i in range(y_train.shape[0])]), tasks=['Class'])\n",
    "        test_set = dc.data.DiskDataset.from_numpy(X=X_test.to_numpy(), y=y_test.to_numpy(), w=np.ones_like(y_test), ids=np.array([i for i in range(y_test.shape[0])]), tasks=['Class'])\n",
    "        valid_set = dc.data.DiskDataset.from_numpy(X=X_valid.to_numpy(), y=y_valid.to_numpy(), w=np.ones_like(y_valid), ids=np.array([i for i in range(y_valid.shape[0])]), tasks=['Class'])\n",
    "\n",
    "        model.fit(train_set)\n",
    "        \n",
    "        used_metrics = []\n",
    "        if regression:\n",
    "            if 'squared_error' in metrics or len(metrics) == 0:\n",
    "                used_metrics.append(dc.metrics.Metric(dc.metrics.rms_score, np.mean))\n",
    "\n",
    "        else:\n",
    "            if 'roc_auc' in metrics or len(metrics) == 0:\n",
    "                used_metrics.append(dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean))\n",
    "\n",
    "        print(\"Train-test results using benchamrk methodology:\")\n",
    "        test_metric = model.evaluate(test_set, used_metrics) #, transformers)\n",
    "        valid_metric = model.evaluate(valid_set, used_metrics) #, transformers)\n",
    "\n",
    "        print(test_metric)\n",
    "        print(valid_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test results using benchamrk methodology:\n",
      "{'mean-rms_score': 1.1162288445516653}\n",
      "{'mean-rms_score': 1.5083228855328679}\n",
      "Standard train-test results:\n",
      "1.130448042796647\n",
      "1.4931813570012455\n"
     ]
    }
   ],
   "source": [
    "for model_name, hyperparams in models_with_parameters:\n",
    "    model = model_builder(model_name, hyperparams)\n",
    "    \n",
    "    benchmark_train_and_test(model, X_train, y_train, X_test, y_test, X_valid, y_valid, regression=regression)\n",
    "    train_and_test(model, X_train, y_train, X_test, y_test, X_valid, y_valid, regression=regression)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import Finalized_pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_datasets = [\"BACE_class\", \"BACE_regre\", \"ROR_class\", \"ROR_regre\"]\n",
    "model_name_shorts = {\"DecisionTreeRegressor\": \"dt\", \"DecisionTreeClassifier\": \"dt\", \n",
    "                    \"RandomForestRegressor\": \"rf\", \"RandomForestClassifier\": \"rf\", \n",
    "                    \"LinearRegression\": \"lr\", \"LogisticRegression\": \"lr\", \n",
    "                    \"MLPRegressor\": \"nn\", \"MLPClassifier\": \"nn\",  \n",
    "                    \"GradientBoostingRegressor\": \"gb\", \"GradientBoostingClassifier\": \"gb\",\n",
    "                    \"XGBRegressor\": \"xg\", \"XGBClassifier\": \"xg\", \n",
    "                    \"SVR\": \"sv\", \"SVC\": \"sv\"}\n",
    "    \n",
    "top_models = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Descriptors': [['MLPClassifier', {'hidden_layer_sizes': (100,), 'activation': 'relu', 'alpha': 0.01, 'max_iter': 200.0, 'solver': 'adam'}], ['MLPClassifier', {'hidden_layer_sizes': (50,), 'activation': 'tanh', 'alpha': 0.0001, 'max_iter': 500.0, 'solver': 'adam'}], ['SVC', {'C': 0.01, 'kernel': 'poly', 'gamma': 0.05}], ['SVC', {'C': 1.0, 'kernel': 'rbf', 'gamma': 0.05}]], 'Fingerprints': [['GradientBoostingClassifier', {'max_depth': 3.0, 'loss': 'exponential', 'n_estimators': 200.0, 'learning_rate': 1.0, 'min_impurity_decrease': 0.05}], ['MLPClassifier', {'hidden_layer_sizes': (100,), 'activation': 'relu', 'alpha': 0.001, 'max_iter': 1000.0, 'solver': 'sgd'}]], 'DescFing': [['LogisticRegression', {'C': 1.0, 'penalty': 'l1', 'solver': 'liblinear'}], ['MLPClassifier', {'hidden_layer_sizes': (50, 50), 'activation': 'relu', 'alpha': 0.01, 'max_iter': 500.0, 'solver': 'sgd'}], ['MLPClassifier', {'hidden_layer_sizes': (100,), 'activation': 'tanh', 'alpha': 0.001, 'max_iter': 1000.0, 'solver': 'sgd'}], ['MLPClassifier', {'hidden_layer_sizes': (100,), 'activation': 'relu', 'alpha': 0.0001, 'max_iter': 1000.0, 'solver': 'sgd'}]]}\n",
      "Empty DataFrame\n",
      "Columns: [model, hyperparams, roc_auc, train_roc_auc, accuracy, train_accuracy, precision, train_precision, recall, train_recall, f1, train_f1, split, preprocessing]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "overflow encountered in reduce                    \n",
      "100%|██████████| 1513/1513 [04:10<00:00,  6.04it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "hyperparameter_search() got an unexpected keyword argument 'return_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m combination \u001b[38;5;129;01min\u001b[39;00m setups[key]:\n\u001b[0;32m     90\u001b[0m                         \u001b[38;5;66;03m# model short name, parameters\u001b[39;00m\n\u001b[0;32m     91\u001b[0m         hyperparams \u001b[38;5;241m=\u001b[39m {model_name_shorts[combination[\u001b[38;5;241m0\u001b[39m]]: combination[\u001b[38;5;241m1\u001b[39m]}\n\u001b[1;32m---> 92\u001b[0m         new_df \u001b[38;5;241m=\u001b[39m \u001b[43mFinalized_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m         returned_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([returned_df, new_df])\n\u001b[0;32m     96\u001b[0m returned_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRERUN_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: hyperparameter_search() got an unexpected keyword argument 'return_df'"
     ]
    }
   ],
   "source": [
    "for dataset in main_datasets:\n",
    "    regression = False\n",
    "    if \"regre\" in dataset:\n",
    "        regression = True\n",
    "\n",
    "\n",
    "    df = pd.read_csv(r\"Merged runs/\" + dataset + \".csv\")\n",
    "    \n",
    "    setups = {\"Descriptors\": [], \"Fingerprints\": [], \"DescFing\": []}\n",
    "\n",
    "    for row in df.head(top_models)[['hyperparams', 'preprocessing', 'model']].iterrows():\n",
    "        #print(row)\n",
    "        params = list(row[1])[0].split(\", '\")\n",
    "        params[0] = params[0][2:]\n",
    "        params[-1] = params[-1][:-1]\n",
    "        #print(params)\n",
    "\n",
    "        preprocessing = list(row[1])[1]\n",
    "        model = list(row[1])[2]\n",
    "        #print(preprocessing)\n",
    "\n",
    "        clean_params = {}\n",
    "        \n",
    "        for param in params:\n",
    "            name = param.split(\"': \")[0]\n",
    "            val = param.split(\"': \")[1]\n",
    "            if val[0] == \"'\":\n",
    "                val = val[1:-1]\n",
    "            elif val[0] == \"(\":\n",
    "                if val == \"(50, 50)\": val = (50, 50)\n",
    "                if val == \"(100, )\" or val == \"(100,)\": val = (100, )\n",
    "                if val == \"(50, )\" or val == \"(50,)\": val = (50, )\n",
    "            elif val == \"None\":\n",
    "                val = None\n",
    "            else:\n",
    "                val = float(val)\n",
    "            clean_params[name] = val\n",
    "        #print(model)\n",
    "        #print(preprocessing)\n",
    "        #print(clean_params)\n",
    "        setups[preprocessing].append([model, clean_params])\n",
    "\n",
    "    for key in setups.keys():\n",
    "        setups[key] = sorted(setups[key], key=lambda x:x[0])\n",
    "    print(setups)\n",
    "\n",
    "    returned_df = pd.DataFrame(columns=df.columns)\n",
    "    print(returned_df)\n",
    "\n",
    "    for key in setups.keys():\n",
    "        if key == \"Descriptors\":\n",
    "            calculate_descriptors = True\n",
    "            calculate_fingerprints = False\n",
    "        if name == \"DescFing\":\n",
    "            calculate_descriptors = True\n",
    "            calculate_fingerprints = True\n",
    "        if key == \"Fingerprints\":\n",
    "            calculate_descriptors = False\n",
    "            calculate_fingerprints = True\n",
    "\n",
    "            \n",
    "        if \"BACE\" in dataset:\n",
    "            prepared_dataset = pd.read_csv(r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\split_datasets\\split0.9_bace.csv\")\n",
    "        else:\n",
    "            prepared_dataset = pd.read_csv(r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\split_datasets\\split0.9_ROR_data_1.csv\")\n",
    "        \n",
    "        if not \"pIC50\" in prepared_dataset.columns:\n",
    "            prepared_dataset = Finalized_pipeline.calculate_pIC50(prepared_dataset, \"target\")\n",
    "        \n",
    "        if not regression:\n",
    "            prepared_dataset = Finalized_pipeline.calculate_classification_labels(prepared_dataset, \"pIC50\", threshold=7)\n",
    "            target_column = \"label\"\n",
    "        else:\n",
    "            target_column = \"pIC50\"\n",
    "\n",
    "        if \"mol\" in prepared_dataset.columns:\n",
    "            prepared_dataset.rename(columns={\"mol\": \"SMILES\"}, inplace=True)\n",
    "\n",
    "        if regression:\n",
    "            runtype = \"regression\"\n",
    "        else:\n",
    "            runtype = \"classification\"\n",
    "        \n",
    "        prepared_dataset = Finalized_pipeline.calculate_features(prepared_dataset, calculate_descriptors=calculate_descriptors, \n",
    "                                                                 calculate_fingerprints=calculate_fingerprints, \n",
    "                                                    SMILES_column_name=\"SMILES\", target_column_name=target_column, \n",
    "                                                    split_column_name=\"Split\")\n",
    "\n",
    "        for combination in setups[key]:\n",
    "                            # model short name, parameters\n",
    "            hyperparams = {model_name_shorts[combination[0]]: combination[1]}\n",
    "            new_df = Finalized_pipeline.hyperparameter_search(prepared_dataset, hyperparams, return_df=True)\n",
    "\n",
    "            returned_df = pd.concat([returned_df, new_df])\n",
    "\n",
    "    returned_df.to_csv(\"RERUN_\" + dataset + \".csv\")\n",
    "        \n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

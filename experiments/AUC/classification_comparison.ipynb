{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import (StratifiedKFold, KFold,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "N_SPLITS = 2\n",
    "RANDOM_STATE = 148260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateMorganFingerprint(mol):\n",
    "    mfpgen = AllChem.GetMorganGenerator(radius=2,fpSize=2048)\n",
    "    fingerprint = np.array([mfpgen.GetFingerprintAsNumPy(x) for x in mol])\n",
    "    fingerprint = pd.DataFrame(fingerprint, columns = ['mfp'+str(i) for i in range(fingerprint.shape[1])])\n",
    "    return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateDescriptors(mol):\n",
    "    calc = Calculator(descriptors, ignore_3D=False)\n",
    "    X_mordred = calc.pandas(mol, nproc=1)\n",
    "    X_mordred = X_mordred.select_dtypes(['number'])\n",
    "    #normalize\n",
    "    X_mordred = (X_mordred-X_mordred.min())/(X_mordred.max()-X_mordred.min())\n",
    "    #drop columns wth low std\n",
    "    X_mordred = X_mordred.loc[:,X_mordred.std()>0.01]\n",
    "    return X_mordred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadDatasetCSV(path, threshold=7.0, regression = False):\n",
    "    df = pd.read_csv(path)\n",
    "    df['molecule_from_smiles'] = df['smiles'].apply(Chem.MolFromSmiles)\n",
    "    df['smiles'] = df['smiles'].map(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x)))\n",
    "    df.drop_duplicates('smiles')\n",
    "    df = df.dropna()\n",
    "    if regression:\n",
    "        df['Target'] = df['pIC50']\n",
    "    else:\n",
    "        df['Target'] = df['pIC50'] > threshold\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, approach = 'desc', split = 0.7):\n",
    "    #TODO: support for different approaches - if applicable\n",
    "    if approach == 'desc':\n",
    "        X = CalculateDescriptors(df['molecule_from_smiles'])\n",
    "    else:\n",
    "        X = CalculateMorganFingerprint(df['molecule_from_smiles'])\n",
    "    y = df[\"Target\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1-split), random_state=42)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rf(X, y, n_estimators, max_depth, min_samples_split, min_samples_leaf, regression=False):\n",
    "    if regression:\n",
    "        name = \"RandomForestRegressor\"\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "        cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        name = \"RandomForestClassifier\"\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "        cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'roc_auc'\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    mean_accuracy = scores.mean()\n",
    "    return (f\"{name}-{n_estimators}-{max_depth}-{min_samples_split}-{min_samples_leaf}; {mean_accuracy:.4f}\")\n",
    "\n",
    "def run_lr(X, y, C, penalty, solver, regression=False):\n",
    "    if regression:\n",
    "        name = \"LinearRegression\"\n",
    "        model = LinearRegression()\n",
    "        cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        name = \"LogisticRegression\"\n",
    "        model = LogisticRegression(C=C, penalty=penalty, solver=solver)\n",
    "        cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'roc_auc'\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    mean_accuracy = scores.mean()\n",
    "    return (f\"{name}-{C}-{penalty}-{solver}; {mean_accuracy:.4f}\")\n",
    "\n",
    "def run_nn(X, y, hidden_layer_sizes, activation, alpha, max_iter, regression=False):\n",
    "    if regression:\n",
    "        name = \"MLPRegressor\"\n",
    "        model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, activation=activation, alpha=alpha, max_iter=max_iter)\n",
    "        cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        name = \"MLPClassifier\"\n",
    "        model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, alpha=alpha, max_iter=max_iter)\n",
    "        cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'roc_auc'\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    mean_accuracy = scores.mean()\n",
    "    return (f\"{name}-{hidden_layer_sizes}-{activation}-{alpha}-{max_iter}; {mean_accuracy:.4f}\")\n",
    "\n",
    "def run_gb(X, y, n_estimators, learning_rate, regression=False):\n",
    "    if regression:\n",
    "        name = \"GradientBoostingRegressor\"\n",
    "        model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        name = \"GradientBoostingClassifier\"\n",
    "        model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "        cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'roc_auc'\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    mean_accuracy = scores.mean()\n",
    "    return (f\"{name}-{n_estimators}-{learning_rate}; {mean_accuracy:.4f}\")\n",
    "\n",
    "def run_svm(X, y, c, d, e, regression=False):\n",
    "    if regression:\n",
    "        name = \"SVR\"\n",
    "        model = SVR(C=c, degree=d, epsilon=e, kernel=\"poly\")\n",
    "        cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        name = \"SVC\"\n",
    "        model = SVC(C=c, degree=d, kernel=\"poly\") ### Epsilon is ignored\n",
    "        cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring = 'roc_auc'\n",
    "    print(\"Eval\\n\")\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    mean_accuracy = scores.mean()\n",
    "    return (f\"{name}-{c}-{d}-{e}; {mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(X, y, regression=False):\n",
    "    results = []\n",
    "\n",
    "    print(\"Run\")\n",
    "\n",
    "    #### -----\n",
    "\n",
    "    param_grid_rf={\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    param_combinations = list(product(*param_grid_rf.values()))\n",
    "    for combination in param_combinations:\n",
    "        n, m, s, l = combination\n",
    "        results.append(run_rf(X, y, n, m, s, l, regression))\n",
    "        print(results[-1])\n",
    "    ### -----\n",
    "\n",
    "    param_grid_lr = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    if regression:\n",
    "        param_grid_lr = {\n",
    "            'C': [0.001],\n",
    "            'penalty': ['l1'],\n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "    param_combinations = list(product(*param_grid_lr.values()))\n",
    "    for combination in param_combinations:\n",
    "        C, p, s = combination\n",
    "        results.append(run_lr(X, y, C, p, s, regression))\n",
    "        print(results[-1])\n",
    "    ### -----\n",
    "\n",
    "    param_grid_mlp = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'max_iter': [200, 500, 1000]\n",
    "    }\n",
    "    param_combinations = list(product(*param_grid_mlp.values()))\n",
    "    for combination in param_combinations:\n",
    "        h, ac, a, i = combination\n",
    "        results.append(run_nn(X, y, h, ac, a, i, regression))\n",
    "        print(results[-1])\n",
    "    ### -----\n",
    "\n",
    "    param_grid_gb={\n",
    "        'n_estimators': [10, 100, 200], \n",
    "        'learning_rate': [0.1,0.5,1.0,2.0]\n",
    "    }\n",
    "    param_combinations = list(product(*param_grid_gb.values()))\n",
    "    for combination in param_combinations:\n",
    "        n, lr = combination\n",
    "        results.append(run_gb(X, y, n, lr, regression))\n",
    "        print(results[-1])\n",
    "    ### -----\n",
    "    \n",
    "    param_grid_svm = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'degree': [2, 3, 4, 5],\n",
    "        'epsilon': [\"no epsilon\"]\n",
    "    }\n",
    "    \n",
    "    if regression:\n",
    "        param_grid_svm = {\n",
    "            'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'degree': [2, 3, 4, 5],\n",
    "            'epsilon': [0.01, 0.1, 1]\n",
    "        }\n",
    "        param_combinations = list(product(*param_grid_svm.values()))\n",
    "        for combination in param_combinations:\n",
    "            c, d, e = combination\n",
    "            results.append(run_svm(X, y, c, d, e, regression))\n",
    "            print(results[-1])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>pIC50</th>\n",
       "      <th>molecule_from_smiles</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cc1ccccc1-c1ccc2nc(N)c(C[C@@H](C)C(=O)N[C@@H]3...</td>\n",
       "      <td>9.154901</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x000002490FD...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCO[C@H]1C[NH2+][C@@H]([C@@H](O)[C@H](Cc2cc(F...</td>\n",
       "      <td>8.853872</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x000002490FD...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...</td>\n",
       "      <td>8.698970</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x000002490FD...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCOC[C@@H](Oc1cc(C[C@@H]2CS(=O)(=O)C[C@H]([NH2...</td>\n",
       "      <td>8.698970</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x000002490FD...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...</td>\n",
       "      <td>8.698970</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x000002490FD...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles     pIC50   \n",
       "0  Cc1ccccc1-c1ccc2nc(N)c(C[C@@H](C)C(=O)N[C@@H]3...  9.154901  \\\n",
       "1  CCCO[C@H]1C[NH2+][C@@H]([C@@H](O)[C@H](Cc2cc(F...  8.853872   \n",
       "2  CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...  8.698970   \n",
       "3  CCOC[C@@H](Oc1cc(C[C@@H]2CS(=O)(=O)C[C@H]([NH2...  8.698970   \n",
       "4  CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...  8.698970   \n",
       "\n",
       "                                molecule_from_smiles  Target  \n",
       "0  <rdkit.Chem.rdchem.Mol object at 0x000002490FD...    True  \n",
       "1  <rdkit.Chem.rdchem.Mol object at 0x000002490FD...    True  \n",
       "2  <rdkit.Chem.rdchem.Mol object at 0x000002490FD...    True  \n",
       "3  <rdkit.Chem.rdchem.Mol object at 0x000002490FD...    True  \n",
       "4  <rdkit.Chem.rdchem.Mol object at 0x000002490FD...    True  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_classification = LoadDatasetCSV(r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\data\\processed\\simple_input_data.csv\")\n",
    "data_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>pIC50</th>\n",
       "      <th>molecule_from_smiles</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cc1ccccc1-c1ccc2nc(N)c(C[C@@H](C)C(=O)N[C@@H]3...</td>\n",
       "      <td>9.154901</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x00000249122...</td>\n",
       "      <td>9.154901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCO[C@H]1C[NH2+][C@@H]([C@@H](O)[C@H](Cc2cc(F...</td>\n",
       "      <td>8.853872</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x00000249122...</td>\n",
       "      <td>8.853872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...</td>\n",
       "      <td>8.698970</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x00000249122...</td>\n",
       "      <td>8.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCOC[C@@H](Oc1cc(C[C@@H]2CS(=O)(=O)C[C@H]([NH2...</td>\n",
       "      <td>8.698970</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x00000249122...</td>\n",
       "      <td>8.698970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...</td>\n",
       "      <td>8.698970</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x00000249122...</td>\n",
       "      <td>8.698970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles     pIC50   \n",
       "0  Cc1ccccc1-c1ccc2nc(N)c(C[C@@H](C)C(=O)N[C@@H]3...  9.154901  \\\n",
       "1  CCCO[C@H]1C[NH2+][C@@H]([C@@H](O)[C@H](Cc2cc(F...  8.853872   \n",
       "2  CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...  8.698970   \n",
       "3  CCOC[C@@H](Oc1cc(C[C@@H]2CS(=O)(=O)C[C@H]([NH2...  8.698970   \n",
       "4  CCc1cn2c3c(cc(C(=O)N[C@@H](Cc4ccccc4)[C@H](O)C...  8.698970   \n",
       "\n",
       "                                molecule_from_smiles    Target  \n",
       "0  <rdkit.Chem.rdchem.Mol object at 0x00000249122...  9.154901  \n",
       "1  <rdkit.Chem.rdchem.Mol object at 0x00000249122...  8.853872  \n",
       "2  <rdkit.Chem.rdchem.Mol object at 0x00000249122...  8.698970  \n",
       "3  <rdkit.Chem.rdchem.Mol object at 0x00000249122...  8.698970  \n",
       "4  <rdkit.Chem.rdchem.Mol object at 0x00000249122...  8.698970  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_regression = LoadDatasetCSV(r\"C:\\Users\\wojci\\Documents\\GitHub\\czasteczkowa-inzynierka\\experiments\\data\\processed\\simple_input_data.csv\", regression=True)\n",
    "data_regression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_desc_classification, y_train_desc_classification, X_test_desc_classification, y_test_desc_classification = split_data(data_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABCGG</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>SpDiam_A</th>\n",
       "      <th>SpAD_A</th>\n",
       "      <th>SpMAD_A</th>\n",
       "      <th>LogEE_A</th>\n",
       "      <th>...</th>\n",
       "      <th>SRW10</th>\n",
       "      <th>TSRW10</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WPath</th>\n",
       "      <th>WPol</th>\n",
       "      <th>Zagreb1</th>\n",
       "      <th>Zagreb2</th>\n",
       "      <th>mZagreb1</th>\n",
       "      <th>mZagreb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.571986</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.582398</td>\n",
       "      <td>0.456539</td>\n",
       "      <td>0.479510</td>\n",
       "      <td>0.582398</td>\n",
       "      <td>0.446868</td>\n",
       "      <td>0.792690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847445</td>\n",
       "      <td>0.521445</td>\n",
       "      <td>0.588711</td>\n",
       "      <td>0.407040</td>\n",
       "      <td>0.239499</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.591743</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.565881</td>\n",
       "      <td>0.564214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.366165</td>\n",
       "      <td>0.341732</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.373272</td>\n",
       "      <td>0.503185</td>\n",
       "      <td>0.528503</td>\n",
       "      <td>0.373272</td>\n",
       "      <td>0.582010</td>\n",
       "      <td>0.631118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719285</td>\n",
       "      <td>0.341022</td>\n",
       "      <td>0.362488</td>\n",
       "      <td>0.351430</td>\n",
       "      <td>0.084586</td>\n",
       "      <td>0.381295</td>\n",
       "      <td>0.376147</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.280719</td>\n",
       "      <td>0.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>0.234412</td>\n",
       "      <td>0.223823</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.216149</td>\n",
       "      <td>0.592990</td>\n",
       "      <td>0.622827</td>\n",
       "      <td>0.216149</td>\n",
       "      <td>0.374746</td>\n",
       "      <td>0.486042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694361</td>\n",
       "      <td>0.228140</td>\n",
       "      <td>0.220422</td>\n",
       "      <td>0.603534</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.287770</td>\n",
       "      <td>0.256881</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.241499</td>\n",
       "      <td>0.194084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.228498</td>\n",
       "      <td>0.219951</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>0.555507</td>\n",
       "      <td>0.578297</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>0.561592</td>\n",
       "      <td>0.477725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633569</td>\n",
       "      <td>0.333643</td>\n",
       "      <td>0.212989</td>\n",
       "      <td>0.492783</td>\n",
       "      <td>0.025660</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.156298</td>\n",
       "      <td>0.184704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.186547</td>\n",
       "      <td>0.188806</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.178725</td>\n",
       "      <td>0.656629</td>\n",
       "      <td>0.563294</td>\n",
       "      <td>0.178725</td>\n",
       "      <td>0.527251</td>\n",
       "      <td>0.423776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616091</td>\n",
       "      <td>0.312119</td>\n",
       "      <td>0.180768</td>\n",
       "      <td>0.622083</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>0.206422</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.177164</td>\n",
       "      <td>0.156566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>0.331611</td>\n",
       "      <td>0.321370</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322084</td>\n",
       "      <td>0.421017</td>\n",
       "      <td>0.442201</td>\n",
       "      <td>0.322084</td>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.595470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696437</td>\n",
       "      <td>0.312289</td>\n",
       "      <td>0.335223</td>\n",
       "      <td>0.362681</td>\n",
       "      <td>0.070607</td>\n",
       "      <td>0.345324</td>\n",
       "      <td>0.344037</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.321097</td>\n",
       "      <td>0.308081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0.309270</td>\n",
       "      <td>0.325988</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.309149</td>\n",
       "      <td>0.273714</td>\n",
       "      <td>0.287486</td>\n",
       "      <td>0.309149</td>\n",
       "      <td>0.386712</td>\n",
       "      <td>0.572930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573711</td>\n",
       "      <td>0.295718</td>\n",
       "      <td>0.306415</td>\n",
       "      <td>0.137594</td>\n",
       "      <td>0.063828</td>\n",
       "      <td>0.294964</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.316847</td>\n",
       "      <td>0.311688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.283667</td>\n",
       "      <td>0.268915</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.290459</td>\n",
       "      <td>0.861105</td>\n",
       "      <td>0.795958</td>\n",
       "      <td>0.290459</td>\n",
       "      <td>0.927731</td>\n",
       "      <td>0.549717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790551</td>\n",
       "      <td>0.402997</td>\n",
       "      <td>0.241079</td>\n",
       "      <td>0.499394</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>0.359712</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.134660</td>\n",
       "      <td>0.233045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0.331611</td>\n",
       "      <td>0.321370</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322084</td>\n",
       "      <td>0.421017</td>\n",
       "      <td>0.442201</td>\n",
       "      <td>0.322084</td>\n",
       "      <td>0.403183</td>\n",
       "      <td>0.595470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.696437</td>\n",
       "      <td>0.312289</td>\n",
       "      <td>0.335223</td>\n",
       "      <td>0.362681</td>\n",
       "      <td>0.070607</td>\n",
       "      <td>0.345324</td>\n",
       "      <td>0.344037</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.321097</td>\n",
       "      <td>0.308081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0.262532</td>\n",
       "      <td>0.248364</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.245483</td>\n",
       "      <td>0.797722</td>\n",
       "      <td>0.723946</td>\n",
       "      <td>0.245483</td>\n",
       "      <td>0.465557</td>\n",
       "      <td>0.521640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729421</td>\n",
       "      <td>0.375545</td>\n",
       "      <td>0.255968</td>\n",
       "      <td>0.315944</td>\n",
       "      <td>0.037159</td>\n",
       "      <td>0.302158</td>\n",
       "      <td>0.288991</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.213679</td>\n",
       "      <td>0.217893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1063 rows Ã— 1222 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ABC     ABCGG  nAcid  nBase   SpAbs_A   SpMax_A  SpDiam_A   \n",
       "209   0.582920  0.571986   0.00   0.00  0.582398  0.456539  0.479510  \\\n",
       "613   0.366165  0.341732   0.00   0.25  0.373272  0.503185  0.528503   \n",
       "1131  0.234412  0.223823   0.00   0.50  0.216149  0.592990  0.622827   \n",
       "140   0.228498  0.219951   0.25   0.50  0.216418  0.555507  0.578297   \n",
       "1094  0.186547  0.188806   0.00   0.75  0.178725  0.656629  0.563294   \n",
       "...        ...       ...    ...    ...       ...       ...       ...   \n",
       "1130  0.331611  0.321370   0.00   0.25  0.322084  0.421017  0.442201   \n",
       "1294  0.309270  0.325988   0.00   0.00  0.309149  0.273714  0.287486   \n",
       "860   0.283667  0.268915   0.00   1.00  0.290459  0.861105  0.795958   \n",
       "1459  0.331611  0.321370   0.00   0.25  0.322084  0.421017  0.442201   \n",
       "1126  0.262532  0.248364   0.00   0.25  0.245483  0.797722  0.723946   \n",
       "\n",
       "        SpAD_A   SpMAD_A   LogEE_A  ...     SRW10    TSRW10        MW   \n",
       "209   0.582398  0.446868  0.792690  ...  0.847445  0.521445  0.588711  \\\n",
       "613   0.373272  0.582010  0.631118  ...  0.719285  0.341022  0.362488   \n",
       "1131  0.216149  0.374746  0.486042  ...  0.694361  0.228140  0.220422   \n",
       "140   0.216418  0.561592  0.477725  ...  0.633569  0.333643  0.212989   \n",
       "1094  0.178725  0.527251  0.423776  ...  0.616091  0.312119  0.180768   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1130  0.322084  0.403183  0.595470  ...  0.696437  0.312289  0.335223   \n",
       "1294  0.309149  0.386712  0.572930  ...  0.573711  0.295718  0.306415   \n",
       "860   0.290459  0.927731  0.549717  ...  0.790551  0.402997  0.241079   \n",
       "1459  0.322084  0.403183  0.595470  ...  0.696437  0.312289  0.335223   \n",
       "1126  0.245483  0.465557  0.521640  ...  0.729421  0.375545  0.255968   \n",
       "\n",
       "           AMW     WPath      WPol   Zagreb1  Zagreb2  mZagreb1  mZagreb2  \n",
       "209   0.407040  0.239499  0.604317  0.591743    0.596  0.565881  0.564214  \n",
       "613   0.351430  0.084586  0.381295  0.376147    0.384  0.280719  0.355700  \n",
       "1131  0.603534  0.032005  0.287770  0.256881    0.274  0.241499  0.194084  \n",
       "140   0.492783  0.025660  0.230216  0.247706    0.260  0.156298  0.184704  \n",
       "1094  0.622083  0.020099  0.230216  0.206422    0.226  0.177164  0.156566  \n",
       "...        ...       ...       ...       ...      ...       ...       ...  \n",
       "1130  0.362681  0.070607  0.345324  0.344037    0.348  0.321097  0.308081  \n",
       "1294  0.137594  0.063828  0.294964  0.302752    0.296  0.316847  0.311688  \n",
       "860   0.499394  0.034358  0.359712  0.321101    0.364  0.134660  0.233045  \n",
       "1459  0.362681  0.070607  0.345324  0.344037    0.348  0.321097  0.308081  \n",
       "1126  0.315944  0.037159  0.302158  0.288991    0.310  0.213679  0.217893  \n",
       "\n",
       "[1063 rows x 1222 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_desc_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_desc_regression, y_train_desc_regression, X_test_desc_regression, y_test_desc_regression = split_data(data_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fp_classification, y_train_fp_classification, X_test_fp_classification, y_test_fp_classification = split_data(data_classification, approach = 'fp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fp_regression, y_train_fp_regression, X_test_fp_regression, y_test_fp_regression = split_data(data_regression, approach = 'fp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_configured(regression=False, fingerprint=False, pca=False):\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    if fingerprint:\n",
    "        if regression:\n",
    "            X = sc.fit_transform(pd.concat([X_train_fp_regression, X_test_fp_regression]))\n",
    "            y = pd.concat([y_train_fp_regression, y_test_fp_regression])\n",
    "            \n",
    "        else:\n",
    "            X = sc.fit_transform(pd.concat([X_train_fp_classification, X_test_fp_classification]))\n",
    "            y = pd.concat([y_train_fp_classification, y_test_fp_classification])\n",
    "\n",
    "    else:\n",
    "        if regression:\n",
    "            X = sc.fit_transform(pd.concat([X_train_desc_regression, X_test_desc_regression]))\n",
    "            y = pd.concat([y_train_desc_regression, y_test_desc_regression])\n",
    "            \n",
    "        else:\n",
    "            X = sc.fit_transform(pd.concat([X_train_desc_classification, X_test_desc_classification]))\n",
    "            y = pd.concat([y_train_desc_classification, y_test_desc_classification])\n",
    "    \n",
    "    if pca:\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X = pca.fit_transform(X)\n",
    "\n",
    "    results = run_all(X, y, regression=regression)\n",
    "\n",
    "    csv_path = \"all_comparison_results_\"\n",
    "    if regression: csv_path += \"regression\" \n",
    "    else: csv_path += \"classification\"\n",
    "    if fingerprint: csv_path += \"_fingerprints\"\n",
    "    else: csv_path += \"_descriptors\"\n",
    "    csv_path += \"_sc\"\n",
    "    if pca: csv_path += \"pca\"\n",
    "    csv_path += \"_svm.csv\"\n",
    "\n",
    "    data_tuples = [tuple(item.split('; ')) for item in results]\n",
    "    df = pd.DataFrame(data_tuples, columns=['Classifier', 'Accuracy'])\n",
    "    df.to_csv(csv_path)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run\n",
      "RandomForestClassifier-50-None-2-1; 0.8767\n",
      "RandomForestClassifier-50-None-2-2; 0.8761\n",
      "RandomForestClassifier-50-None-2-4; 0.8758\n",
      "RandomForestClassifier-50-None-5-1; 0.8759\n",
      "RandomForestClassifier-50-None-5-2; 0.8758\n",
      "RandomForestClassifier-50-None-5-4; 0.8768\n",
      "RandomForestClassifier-50-None-10-1; 0.8765\n",
      "RandomForestClassifier-50-None-10-2; 0.8739\n",
      "RandomForestClassifier-50-None-10-4; 0.8743\n",
      "RandomForestClassifier-50-10-2-1; 0.8748\n",
      "RandomForestClassifier-50-10-2-2; 0.8754\n",
      "RandomForestClassifier-50-10-2-4; 0.8752\n",
      "RandomForestClassifier-50-10-5-1; 0.8807\n",
      "RandomForestClassifier-50-10-5-2; 0.8749\n",
      "RandomForestClassifier-50-10-5-4; 0.8753\n",
      "RandomForestClassifier-50-10-10-1; 0.8771\n",
      "RandomForestClassifier-50-10-10-2; 0.8752\n",
      "RandomForestClassifier-50-10-10-4; 0.8735\n",
      "RandomForestClassifier-50-20-2-1; 0.8782\n",
      "RandomForestClassifier-50-20-2-2; 0.8762\n",
      "RandomForestClassifier-50-20-2-4; 0.8727\n",
      "RandomForestClassifier-50-20-5-1; 0.8765\n",
      "RandomForestClassifier-50-20-5-2; 0.8798\n",
      "RandomForestClassifier-50-20-5-4; 0.8766\n",
      "RandomForestClassifier-50-20-10-1; 0.8789\n",
      "RandomForestClassifier-50-20-10-2; 0.8746\n",
      "RandomForestClassifier-50-20-10-4; 0.8752\n",
      "RandomForestClassifier-100-None-2-1; 0.8791\n",
      "RandomForestClassifier-100-None-2-2; 0.8783\n",
      "RandomForestClassifier-100-None-2-4; 0.8803\n",
      "RandomForestClassifier-100-None-5-1; 0.8792\n",
      "RandomForestClassifier-100-None-5-2; 0.8808\n",
      "RandomForestClassifier-100-None-5-4; 0.8771\n",
      "RandomForestClassifier-100-None-10-1; 0.8779\n",
      "RandomForestClassifier-100-None-10-2; 0.8781\n",
      "RandomForestClassifier-100-None-10-4; 0.8773\n",
      "RandomForestClassifier-100-10-2-1; 0.8775\n",
      "RandomForestClassifier-100-10-2-2; 0.8778\n",
      "RandomForestClassifier-100-10-2-4; 0.8777\n",
      "RandomForestClassifier-100-10-5-1; 0.8776\n",
      "RandomForestClassifier-100-10-5-2; 0.8771\n",
      "RandomForestClassifier-100-10-5-4; 0.8745\n",
      "RandomForestClassifier-100-10-10-1; 0.8799\n",
      "RandomForestClassifier-100-10-10-2; 0.8803\n",
      "RandomForestClassifier-100-10-10-4; 0.8790\n",
      "RandomForestClassifier-100-20-2-1; 0.8824\n",
      "RandomForestClassifier-100-20-2-2; 0.8815\n",
      "RandomForestClassifier-100-20-2-4; 0.8751\n",
      "RandomForestClassifier-100-20-5-1; 0.8771\n",
      "RandomForestClassifier-100-20-5-2; 0.8782\n",
      "RandomForestClassifier-100-20-5-4; 0.8783\n",
      "RandomForestClassifier-100-20-10-1; 0.8787\n",
      "RandomForestClassifier-100-20-10-2; 0.8794\n",
      "RandomForestClassifier-100-20-10-4; 0.8760\n",
      "LogisticRegression-0.001-l1-liblinear; 0.5000\n",
      "LogisticRegression-0.001-l1-saga; 0.5000\n",
      "LogisticRegression-0.001-l2-liblinear; 0.8567\n",
      "LogisticRegression-0.001-l2-saga; 0.8572\n",
      "LogisticRegression-0.01-l1-liblinear; 0.7848\n",
      "LogisticRegression-0.01-l1-saga; 0.7853\n",
      "LogisticRegression-0.01-l2-liblinear; 0.8763\n",
      "LogisticRegression-0.01-l2-saga; 0.8761\n",
      "LogisticRegression-0.1-l1-liblinear; 0.8717\n",
      "LogisticRegression-0.1-l1-saga; 0.8671\n",
      "LogisticRegression-0.1-l2-liblinear; 0.8698\n",
      "LogisticRegression-0.1-l2-saga; 0.8748\n",
      "LogisticRegression-1-l1-liblinear; 0.8673\n",
      "LogisticRegression-1-l1-saga; 0.8749\n",
      "LogisticRegression-1-l2-liblinear; 0.8525\n",
      "LogisticRegression-1-l2-saga; 0.8745\n",
      "LogisticRegression-10-l1-liblinear; 0.8301\n",
      "LogisticRegression-10-l1-saga; 0.8745\n",
      "LogisticRegression-10-l2-liblinear; 0.8330\n",
      "LogisticRegression-10-l2-saga; 0.8744\n",
      "LogisticRegression-100-l1-liblinear; 0.8187\n",
      "LogisticRegression-100-l1-saga; 0.8743\n",
      "LogisticRegression-100-l2-liblinear; 0.8213\n",
      "LogisticRegression-100-l2-saga; 0.8744\n",
      "MLPClassifier-(50,)-relu-0.0001-200; 0.8674\n",
      "MLPClassifier-(50,)-relu-0.0001-500; 0.8608\n",
      "MLPClassifier-(50,)-relu-0.0001-1000; 0.8625\n",
      "MLPClassifier-(50,)-relu-0.001-200; 0.8620\n",
      "MLPClassifier-(50,)-relu-0.001-500; 0.8666\n",
      "MLPClassifier-(50,)-relu-0.001-1000; 0.8642\n",
      "MLPClassifier-(50,)-relu-0.01-200; 0.8644\n",
      "MLPClassifier-(50,)-relu-0.01-500; 0.8645\n",
      "MLPClassifier-(50,)-relu-0.01-1000; 0.8666\n",
      "MLPClassifier-(50,)-tanh-0.0001-200; 0.8738\n",
      "MLPClassifier-(50,)-tanh-0.0001-500; 0.8723\n",
      "MLPClassifier-(50,)-tanh-0.0001-1000; 0.8767\n",
      "MLPClassifier-(50,)-tanh-0.001-200; 0.8735\n",
      "MLPClassifier-(50,)-tanh-0.001-500; 0.8734\n",
      "MLPClassifier-(50,)-tanh-0.001-1000; 0.8732\n",
      "MLPClassifier-(50,)-tanh-0.01-200; 0.8766\n",
      "MLPClassifier-(50,)-tanh-0.01-500; 0.8712\n",
      "MLPClassifier-(50,)-tanh-0.01-1000; 0.8737\n",
      "MLPClassifier-(100,)-relu-0.0001-200; 0.8668\n",
      "MLPClassifier-(100,)-relu-0.0001-500; 0.8655\n",
      "MLPClassifier-(100,)-relu-0.0001-1000; 0.8663\n",
      "MLPClassifier-(100,)-relu-0.001-200; 0.8655\n",
      "MLPClassifier-(100,)-relu-0.001-500; 0.8646\n",
      "MLPClassifier-(100,)-relu-0.001-1000; 0.8652\n",
      "MLPClassifier-(100,)-relu-0.01-200; 0.8632\n",
      "MLPClassifier-(100,)-relu-0.01-500; 0.8691\n",
      "MLPClassifier-(100,)-relu-0.01-1000; 0.8654\n",
      "MLPClassifier-(100,)-tanh-0.0001-200; 0.8742\n",
      "MLPClassifier-(100,)-tanh-0.0001-500; 0.8713\n",
      "MLPClassifier-(100,)-tanh-0.0001-1000; 0.8661\n",
      "MLPClassifier-(100,)-tanh-0.001-200; 0.8728\n",
      "MLPClassifier-(100,)-tanh-0.001-500; 0.8714\n",
      "MLPClassifier-(100,)-tanh-0.001-1000; 0.8748\n",
      "MLPClassifier-(100,)-tanh-0.01-200; 0.8734\n",
      "MLPClassifier-(100,)-tanh-0.01-500; 0.8724\n",
      "MLPClassifier-(100,)-tanh-0.01-1000; 0.8688\n",
      "MLPClassifier-(50, 50)-relu-0.0001-200; 0.8721\n",
      "MLPClassifier-(50, 50)-relu-0.0001-500; 0.8721\n",
      "MLPClassifier-(50, 50)-relu-0.0001-1000; 0.8612\n",
      "MLPClassifier-(50, 50)-relu-0.001-200; 0.8688\n",
      "MLPClassifier-(50, 50)-relu-0.001-500; 0.8749\n",
      "MLPClassifier-(50, 50)-relu-0.001-1000; 0.8609\n",
      "MLPClassifier-(50, 50)-relu-0.01-200; 0.8751\n",
      "MLPClassifier-(50, 50)-relu-0.01-500; 0.8608\n",
      "MLPClassifier-(50, 50)-relu-0.01-1000; 0.8700\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-200; 0.8748\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-500; 0.8710\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-1000; 0.8687\n",
      "MLPClassifier-(50, 50)-tanh-0.001-200; 0.8688\n",
      "MLPClassifier-(50, 50)-tanh-0.001-500; 0.8734\n",
      "MLPClassifier-(50, 50)-tanh-0.001-1000; 0.8704\n",
      "MLPClassifier-(50, 50)-tanh-0.01-200; 0.8636\n",
      "MLPClassifier-(50, 50)-tanh-0.01-500; 0.8736\n",
      "MLPClassifier-(50, 50)-tanh-0.01-1000; 0.8717\n",
      "GradientBoostingClassifier-10-0.1; 0.8464\n",
      "GradientBoostingClassifier-10-0.5; 0.8549\n",
      "GradientBoostingClassifier-10-1.0; 0.8201\n",
      "GradientBoostingClassifier-10-2.0; 0.7264\n",
      "GradientBoostingClassifier-100-0.1; 0.8793\n",
      "GradientBoostingClassifier-100-0.5; 0.8642\n",
      "GradientBoostingClassifier-100-1.0; 0.8486\n",
      "GradientBoostingClassifier-100-2.0; 0.7302\n",
      "GradientBoostingClassifier-200-0.1; 0.8776\n",
      "GradientBoostingClassifier-200-0.5; 0.8629\n",
      "GradientBoostingClassifier-200-1.0; 0.8476\n",
      "GradientBoostingClassifier-200-2.0; 0.7293\n",
      "Run\n",
      "RandomForestClassifier-50-None-2-1; 0.8555\n",
      "RandomForestClassifier-50-None-2-2; 0.8681\n",
      "RandomForestClassifier-50-None-2-4; 0.8664\n",
      "RandomForestClassifier-50-None-5-1; 0.8726\n",
      "RandomForestClassifier-50-None-5-2; 0.8620\n",
      "RandomForestClassifier-50-None-5-4; 0.8632\n",
      "RandomForestClassifier-50-None-10-1; 0.8612\n",
      "RandomForestClassifier-50-None-10-2; 0.8665\n",
      "RandomForestClassifier-50-None-10-4; 0.8721\n",
      "RandomForestClassifier-50-10-2-1; 0.8641\n",
      "RandomForestClassifier-50-10-2-2; 0.8630\n",
      "RandomForestClassifier-50-10-2-4; 0.8636\n",
      "RandomForestClassifier-50-10-5-1; 0.8631\n",
      "RandomForestClassifier-50-10-5-2; 0.8621\n",
      "RandomForestClassifier-50-10-5-4; 0.8651\n",
      "RandomForestClassifier-50-10-10-1; 0.8651\n",
      "RandomForestClassifier-50-10-10-2; 0.8619\n",
      "RandomForestClassifier-50-10-10-4; 0.8632\n",
      "RandomForestClassifier-50-20-2-1; 0.8616\n",
      "RandomForestClassifier-50-20-2-2; 0.8694\n",
      "RandomForestClassifier-50-20-2-4; 0.8679\n",
      "RandomForestClassifier-50-20-5-1; 0.8647\n",
      "RandomForestClassifier-50-20-5-2; 0.8623\n",
      "RandomForestClassifier-50-20-5-4; 0.8675\n",
      "RandomForestClassifier-50-20-10-1; 0.8630\n",
      "RandomForestClassifier-50-20-10-2; 0.8708\n",
      "RandomForestClassifier-50-20-10-4; 0.8650\n",
      "RandomForestClassifier-100-None-2-1; 0.8620\n",
      "RandomForestClassifier-100-None-2-2; 0.8718\n",
      "RandomForestClassifier-100-None-2-4; 0.8737\n",
      "RandomForestClassifier-100-None-5-1; 0.8679\n",
      "RandomForestClassifier-100-None-5-2; 0.8688\n",
      "RandomForestClassifier-100-None-5-4; 0.8667\n",
      "RandomForestClassifier-100-None-10-1; 0.8711\n",
      "RandomForestClassifier-100-None-10-2; 0.8744\n",
      "RandomForestClassifier-100-None-10-4; 0.8672\n",
      "RandomForestClassifier-100-10-2-1; 0.8664\n",
      "RandomForestClassifier-100-10-2-2; 0.8676\n",
      "RandomForestClassifier-100-10-2-4; 0.8728\n",
      "RandomForestClassifier-100-10-5-1; 0.8654\n",
      "RandomForestClassifier-100-10-5-2; 0.8695\n",
      "RandomForestClassifier-100-10-5-4; 0.8659\n",
      "RandomForestClassifier-100-10-10-1; 0.8703\n",
      "RandomForestClassifier-100-10-10-2; 0.8680\n",
      "RandomForestClassifier-100-10-10-4; 0.8730\n",
      "RandomForestClassifier-100-20-2-1; 0.8706\n",
      "RandomForestClassifier-100-20-2-2; 0.8679\n",
      "RandomForestClassifier-100-20-2-4; 0.8738\n",
      "RandomForestClassifier-100-20-5-1; 0.8664\n",
      "RandomForestClassifier-100-20-5-2; 0.8715\n",
      "RandomForestClassifier-100-20-5-4; 0.8731\n",
      "RandomForestClassifier-100-20-10-1; 0.8689\n",
      "RandomForestClassifier-100-20-10-2; 0.8676\n",
      "RandomForestClassifier-100-20-10-4; 0.8726\n",
      "LogisticRegression-0.001-l1-liblinear; 0.7315\n",
      "LogisticRegression-0.001-l1-saga; 0.7318\n",
      "LogisticRegression-0.001-l2-liblinear; 0.8478\n",
      "LogisticRegression-0.001-l2-saga; 0.8483\n",
      "LogisticRegression-0.01-l1-liblinear; 0.8092\n",
      "LogisticRegression-0.01-l1-saga; 0.8089\n",
      "LogisticRegression-0.01-l2-liblinear; 0.8560\n",
      "LogisticRegression-0.01-l2-saga; 0.8565\n",
      "LogisticRegression-0.1-l1-liblinear; 0.8539\n",
      "LogisticRegression-0.1-l1-saga; 0.8556\n",
      "LogisticRegression-0.1-l2-liblinear; 0.8526\n",
      "LogisticRegression-0.1-l2-saga; 0.8536\n",
      "LogisticRegression-1-l1-liblinear; 0.8508\n",
      "LogisticRegression-1-l1-saga; 0.8536\n",
      "LogisticRegression-1-l2-liblinear; 0.8494\n",
      "LogisticRegression-1-l2-saga; 0.8532\n",
      "LogisticRegression-10-l1-liblinear; 0.8489\n",
      "LogisticRegression-10-l1-saga; 0.8531\n",
      "LogisticRegression-10-l2-liblinear; 0.8486\n",
      "LogisticRegression-10-l2-saga; 0.8531\n",
      "LogisticRegression-100-l1-liblinear; 0.8485\n",
      "LogisticRegression-100-l1-saga; 0.8531\n",
      "LogisticRegression-100-l2-liblinear; 0.8484\n",
      "LogisticRegression-100-l2-saga; 0.8531\n",
      "MLPClassifier-(50,)-relu-0.0001-200; 0.8686\n",
      "MLPClassifier-(50,)-relu-0.0001-500; 0.8611\n",
      "MLPClassifier-(50,)-relu-0.0001-1000; 0.8635\n",
      "MLPClassifier-(50,)-relu-0.001-200; 0.8674\n",
      "MLPClassifier-(50,)-relu-0.001-500; 0.8604\n",
      "MLPClassifier-(50,)-relu-0.001-1000; 0.8614\n",
      "MLPClassifier-(50,)-relu-0.01-200; 0.8638\n",
      "MLPClassifier-(50,)-relu-0.01-500; 0.8601\n",
      "MLPClassifier-(50,)-relu-0.01-1000; 0.8621\n",
      "MLPClassifier-(50,)-tanh-0.0001-200; 0.8687\n",
      "MLPClassifier-(50,)-tanh-0.0001-500; 0.8622\n",
      "MLPClassifier-(50,)-tanh-0.0001-1000; 0.8612\n",
      "MLPClassifier-(50,)-tanh-0.001-200; 0.8719\n",
      "MLPClassifier-(50,)-tanh-0.001-500; 0.8679\n",
      "MLPClassifier-(50,)-tanh-0.001-1000; 0.8667\n",
      "MLPClassifier-(50,)-tanh-0.01-200; 0.8708\n",
      "MLPClassifier-(50,)-tanh-0.01-500; 0.8658\n",
      "MLPClassifier-(50,)-tanh-0.01-1000; 0.8650\n",
      "MLPClassifier-(100,)-relu-0.0001-200; 0.8637\n",
      "MLPClassifier-(100,)-relu-0.0001-500; 0.8610\n",
      "MLPClassifier-(100,)-relu-0.0001-1000; 0.8650\n",
      "MLPClassifier-(100,)-relu-0.001-200; 0.8709\n",
      "MLPClassifier-(100,)-relu-0.001-500; 0.8638\n",
      "MLPClassifier-(100,)-relu-0.001-1000; 0.8638\n",
      "MLPClassifier-(100,)-relu-0.01-200; 0.8699\n",
      "MLPClassifier-(100,)-relu-0.01-500; 0.8709\n",
      "MLPClassifier-(100,)-relu-0.01-1000; 0.8630\n",
      "MLPClassifier-(100,)-tanh-0.0001-200; 0.8728\n",
      "MLPClassifier-(100,)-tanh-0.0001-500; 0.8690\n",
      "MLPClassifier-(100,)-tanh-0.0001-1000; 0.8703\n",
      "MLPClassifier-(100,)-tanh-0.001-200; 0.8746\n",
      "MLPClassifier-(100,)-tanh-0.001-500; 0.8697\n",
      "MLPClassifier-(100,)-tanh-0.001-1000; 0.8738\n",
      "MLPClassifier-(100,)-tanh-0.01-200; 0.8712\n",
      "MLPClassifier-(100,)-tanh-0.01-500; 0.8717\n",
      "MLPClassifier-(100,)-tanh-0.01-1000; 0.8670\n",
      "MLPClassifier-(50, 50)-relu-0.0001-200; 0.8663\n",
      "MLPClassifier-(50, 50)-relu-0.0001-500; 0.8668\n",
      "MLPClassifier-(50, 50)-relu-0.0001-1000; 0.8623\n",
      "MLPClassifier-(50, 50)-relu-0.001-200; 0.8584\n",
      "MLPClassifier-(50, 50)-relu-0.001-500; 0.8652\n",
      "MLPClassifier-(50, 50)-relu-0.001-1000; 0.8586\n",
      "MLPClassifier-(50, 50)-relu-0.01-200; 0.8629\n",
      "MLPClassifier-(50, 50)-relu-0.01-500; 0.8643\n",
      "MLPClassifier-(50, 50)-relu-0.01-1000; 0.8601\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-200; 0.8580\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-500; 0.8563\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-1000; 0.8600\n",
      "MLPClassifier-(50, 50)-tanh-0.001-200; 0.8669\n",
      "MLPClassifier-(50, 50)-tanh-0.001-500; 0.8568\n",
      "MLPClassifier-(50, 50)-tanh-0.001-1000; 0.8619\n",
      "MLPClassifier-(50, 50)-tanh-0.01-200; 0.8504\n",
      "MLPClassifier-(50, 50)-tanh-0.01-500; 0.8594\n",
      "MLPClassifier-(50, 50)-tanh-0.01-1000; 0.8566\n",
      "GradientBoostingClassifier-10-0.1; 0.8253\n",
      "GradientBoostingClassifier-10-0.5; 0.8199\n",
      "GradientBoostingClassifier-10-1.0; 0.8129\n",
      "GradientBoostingClassifier-10-2.0; 0.7257\n",
      "GradientBoostingClassifier-100-0.1; 0.8626\n",
      "GradientBoostingClassifier-100-0.5; 0.8483\n",
      "GradientBoostingClassifier-100-1.0; 0.8437\n",
      "GradientBoostingClassifier-100-2.0; 0.7053\n",
      "GradientBoostingClassifier-200-0.1; 0.8653\n",
      "GradientBoostingClassifier-200-0.5; 0.8529\n",
      "GradientBoostingClassifier-200-1.0; 0.8458\n",
      "GradientBoostingClassifier-200-2.0; 0.7004\n",
      "Run\n",
      "RandomForestClassifier-50-None-2-1; 0.8819\n",
      "RandomForestClassifier-50-None-2-2; 0.8869\n",
      "RandomForestClassifier-50-None-2-4; 0.8823\n",
      "RandomForestClassifier-50-None-5-1; 0.8870\n",
      "RandomForestClassifier-50-None-5-2; 0.8896\n",
      "RandomForestClassifier-50-None-5-4; 0.8771\n",
      "RandomForestClassifier-50-None-10-1; 0.8838\n",
      "RandomForestClassifier-50-None-10-2; 0.8874\n",
      "RandomForestClassifier-50-None-10-4; 0.8827\n",
      "RandomForestClassifier-50-10-2-1; 0.8781\n",
      "RandomForestClassifier-50-10-2-2; 0.8788\n",
      "RandomForestClassifier-50-10-2-4; 0.8742\n",
      "RandomForestClassifier-50-10-5-1; 0.8764\n",
      "RandomForestClassifier-50-10-5-2; 0.8752\n",
      "RandomForestClassifier-50-10-5-4; 0.8703\n",
      "RandomForestClassifier-50-10-10-1; 0.8797\n",
      "RandomForestClassifier-50-10-10-2; 0.8745\n",
      "RandomForestClassifier-50-10-10-4; 0.8716\n",
      "RandomForestClassifier-50-20-2-1; 0.8818\n",
      "RandomForestClassifier-50-20-2-2; 0.8842\n",
      "RandomForestClassifier-50-20-2-4; 0.8777\n",
      "RandomForestClassifier-50-20-5-1; 0.8817\n",
      "RandomForestClassifier-50-20-5-2; 0.8890\n",
      "RandomForestClassifier-50-20-5-4; 0.8808\n",
      "RandomForestClassifier-50-20-10-1; 0.8882\n",
      "RandomForestClassifier-50-20-10-2; 0.8836\n",
      "RandomForestClassifier-50-20-10-4; 0.8799\n",
      "RandomForestClassifier-100-None-2-1; 0.8789\n",
      "RandomForestClassifier-100-None-2-2; 0.8865\n",
      "RandomForestClassifier-100-None-2-4; 0.8799\n",
      "RandomForestClassifier-100-None-5-1; 0.8875\n",
      "RandomForestClassifier-100-None-5-2; 0.8856\n",
      "RandomForestClassifier-100-None-5-4; 0.8791\n",
      "RandomForestClassifier-100-None-10-1; 0.8869\n",
      "RandomForestClassifier-100-None-10-2; 0.8864\n",
      "RandomForestClassifier-100-None-10-4; 0.8791\n",
      "RandomForestClassifier-100-10-2-1; 0.8768\n",
      "RandomForestClassifier-100-10-2-2; 0.8788\n",
      "RandomForestClassifier-100-10-2-4; 0.8719\n",
      "RandomForestClassifier-100-10-5-1; 0.8804\n",
      "RandomForestClassifier-100-10-5-2; 0.8787\n",
      "RandomForestClassifier-100-10-5-4; 0.8745\n",
      "RandomForestClassifier-100-10-10-1; 0.8785\n",
      "RandomForestClassifier-100-10-10-2; 0.8736\n",
      "RandomForestClassifier-100-10-10-4; 0.8713\n",
      "RandomForestClassifier-100-20-2-1; 0.8874\n",
      "RandomForestClassifier-100-20-2-2; 0.8853\n",
      "RandomForestClassifier-100-20-2-4; 0.8812\n",
      "RandomForestClassifier-100-20-5-1; 0.8859\n",
      "RandomForestClassifier-100-20-5-2; 0.8865\n",
      "RandomForestClassifier-100-20-5-4; 0.8777\n",
      "RandomForestClassifier-100-20-10-1; 0.8879\n",
      "RandomForestClassifier-100-20-10-2; 0.8852\n",
      "RandomForestClassifier-100-20-10-4; 0.8784\n",
      "LogisticRegression-0.001-l1-liblinear; 0.5000\n",
      "LogisticRegression-0.001-l1-saga; 0.5000\n",
      "LogisticRegression-0.001-l2-liblinear; 0.8848\n",
      "LogisticRegression-0.001-l2-saga; 0.8866\n",
      "LogisticRegression-0.01-l1-liblinear; 0.5951\n",
      "LogisticRegression-0.01-l1-saga; 0.6026\n",
      "LogisticRegression-0.01-l2-liblinear; 0.8799\n",
      "LogisticRegression-0.01-l2-saga; 0.8819\n",
      "LogisticRegression-0.1-l1-liblinear; 0.8818\n",
      "LogisticRegression-0.1-l1-saga; 0.8861\n",
      "LogisticRegression-0.1-l2-liblinear; 0.8622\n",
      "LogisticRegression-0.1-l2-saga; 0.8717\n",
      "LogisticRegression-1-l1-liblinear; 0.8533\n",
      "LogisticRegression-1-l1-saga; 0.8743\n",
      "LogisticRegression-1-l2-liblinear; 0.8472\n",
      "LogisticRegression-1-l2-saga; 0.8706\n",
      "LogisticRegression-10-l1-liblinear; 0.8347\n",
      "LogisticRegression-10-l1-saga; 0.8713\n",
      "LogisticRegression-10-l2-liblinear; 0.8371\n",
      "LogisticRegression-10-l2-saga; 0.8706\n",
      "LogisticRegression-100-l1-liblinear; 0.8260\n",
      "LogisticRegression-100-l1-saga; 0.8707\n",
      "LogisticRegression-100-l2-liblinear; 0.8232\n",
      "LogisticRegression-100-l2-saga; 0.8705\n",
      "MLPClassifier-(50,)-relu-0.0001-200; 0.8445\n",
      "MLPClassifier-(50,)-relu-0.0001-500; 0.8579\n",
      "MLPClassifier-(50,)-relu-0.0001-1000; 0.8508\n",
      "MLPClassifier-(50,)-relu-0.001-200; 0.8509\n",
      "MLPClassifier-(50,)-relu-0.001-500; 0.8501\n",
      "MLPClassifier-(50,)-relu-0.001-1000; 0.8479\n",
      "MLPClassifier-(50,)-relu-0.01-200; 0.8595\n",
      "MLPClassifier-(50,)-relu-0.01-500; 0.8542\n",
      "MLPClassifier-(50,)-relu-0.01-1000; 0.8598\n",
      "MLPClassifier-(50,)-tanh-0.0001-200; 0.8303\n",
      "MLPClassifier-(50,)-tanh-0.0001-500; 0.8383\n",
      "MLPClassifier-(50,)-tanh-0.0001-1000; 0.8419\n",
      "MLPClassifier-(50,)-tanh-0.001-200; 0.8388\n",
      "MLPClassifier-(50,)-tanh-0.001-500; 0.8452\n",
      "MLPClassifier-(50,)-tanh-0.001-1000; 0.8393\n",
      "MLPClassifier-(50,)-tanh-0.01-200; 0.8457\n",
      "MLPClassifier-(50,)-tanh-0.01-500; 0.8463\n",
      "MLPClassifier-(50,)-tanh-0.01-1000; 0.8494\n",
      "MLPClassifier-(100,)-relu-0.0001-200; 0.8538\n",
      "MLPClassifier-(100,)-relu-0.0001-500; 0.8567\n",
      "MLPClassifier-(100,)-relu-0.0001-1000; 0.8559\n",
      "MLPClassifier-(100,)-relu-0.001-200; 0.8557\n",
      "MLPClassifier-(100,)-relu-0.001-500; 0.8553\n",
      "MLPClassifier-(100,)-relu-0.001-1000; 0.8487\n",
      "MLPClassifier-(100,)-relu-0.01-200; 0.8562\n",
      "MLPClassifier-(100,)-relu-0.01-500; 0.8624\n",
      "MLPClassifier-(100,)-relu-0.01-1000; 0.8562\n",
      "MLPClassifier-(100,)-tanh-0.0001-200; 0.8368\n",
      "MLPClassifier-(100,)-tanh-0.0001-500; 0.8359\n",
      "MLPClassifier-(100,)-tanh-0.0001-1000; 0.8406\n",
      "MLPClassifier-(100,)-tanh-0.001-200; 0.8434\n",
      "MLPClassifier-(100,)-tanh-0.001-500; 0.8382\n",
      "MLPClassifier-(100,)-tanh-0.001-1000; 0.8398\n",
      "MLPClassifier-(100,)-tanh-0.01-200; 0.8467\n",
      "MLPClassifier-(100,)-tanh-0.01-500; 0.8508\n",
      "MLPClassifier-(100,)-tanh-0.01-1000; 0.8467\n",
      "MLPClassifier-(50, 50)-relu-0.0001-200; 0.8612\n",
      "MLPClassifier-(50, 50)-relu-0.0001-500; 0.8532\n",
      "MLPClassifier-(50, 50)-relu-0.0001-1000; 0.8584\n",
      "MLPClassifier-(50, 50)-relu-0.001-200; 0.8597\n",
      "MLPClassifier-(50, 50)-relu-0.001-500; 0.8553\n",
      "MLPClassifier-(50, 50)-relu-0.001-1000; 0.8527\n",
      "MLPClassifier-(50, 50)-relu-0.01-200; 0.8513\n",
      "MLPClassifier-(50, 50)-relu-0.01-500; 0.8570\n",
      "MLPClassifier-(50, 50)-relu-0.01-1000; 0.8558\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-200; 0.8454\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-500; 0.8486\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-1000; 0.8399\n",
      "MLPClassifier-(50, 50)-tanh-0.001-200; 0.8450\n",
      "MLPClassifier-(50, 50)-tanh-0.001-500; 0.8442\n",
      "MLPClassifier-(50, 50)-tanh-0.001-1000; 0.8442\n",
      "MLPClassifier-(50, 50)-tanh-0.01-200; 0.8477\n",
      "MLPClassifier-(50, 50)-tanh-0.01-500; 0.8464\n",
      "MLPClassifier-(50, 50)-tanh-0.01-1000; 0.8471\n",
      "GradientBoostingClassifier-10-0.1; 0.8400\n",
      "GradientBoostingClassifier-10-0.5; 0.8732\n",
      "GradientBoostingClassifier-10-1.0; 0.8439\n",
      "GradientBoostingClassifier-10-2.0; 0.7324\n",
      "GradientBoostingClassifier-100-0.1; 0.8845\n",
      "GradientBoostingClassifier-100-0.5; 0.8705\n",
      "GradientBoostingClassifier-100-1.0; 0.8508\n",
      "GradientBoostingClassifier-100-2.0; 0.6797\n",
      "GradientBoostingClassifier-200-0.1; 0.8869\n",
      "GradientBoostingClassifier-200-0.5; 0.8714\n",
      "GradientBoostingClassifier-200-1.0; 0.8564\n",
      "GradientBoostingClassifier-200-2.0; 0.7470\n",
      "Run\n",
      "RandomForestClassifier-50-None-2-1; 0.8294\n",
      "RandomForestClassifier-50-None-2-2; 0.8396\n",
      "RandomForestClassifier-50-None-2-4; 0.8374\n",
      "RandomForestClassifier-50-None-5-1; 0.8318\n",
      "RandomForestClassifier-50-None-5-2; 0.8380\n",
      "RandomForestClassifier-50-None-5-4; 0.8295\n",
      "RandomForestClassifier-50-None-10-1; 0.8403\n",
      "RandomForestClassifier-50-None-10-2; 0.8448\n",
      "RandomForestClassifier-50-None-10-4; 0.8333\n",
      "RandomForestClassifier-50-10-2-1; 0.8415\n",
      "RandomForestClassifier-50-10-2-2; 0.8494\n",
      "RandomForestClassifier-50-10-2-4; 0.8415\n",
      "RandomForestClassifier-50-10-5-1; 0.8451\n",
      "RandomForestClassifier-50-10-5-2; 0.8418\n",
      "RandomForestClassifier-50-10-5-4; 0.8371\n",
      "RandomForestClassifier-50-10-10-1; 0.8447\n",
      "RandomForestClassifier-50-10-10-2; 0.8420\n",
      "RandomForestClassifier-50-10-10-4; 0.8504\n",
      "RandomForestClassifier-50-20-2-1; 0.8385\n",
      "RandomForestClassifier-50-20-2-2; 0.8339\n",
      "RandomForestClassifier-50-20-2-4; 0.8432\n",
      "RandomForestClassifier-50-20-5-1; 0.8282\n",
      "RandomForestClassifier-50-20-5-2; 0.8404\n",
      "RandomForestClassifier-50-20-5-4; 0.8393\n",
      "RandomForestClassifier-50-20-10-1; 0.8458\n",
      "RandomForestClassifier-50-20-10-2; 0.8543\n",
      "RandomForestClassifier-50-20-10-4; 0.8386\n",
      "RandomForestClassifier-100-None-2-1; 0.8512\n",
      "RandomForestClassifier-100-None-2-2; 0.8475\n",
      "RandomForestClassifier-100-None-2-4; 0.8564\n",
      "RandomForestClassifier-100-None-5-1; 0.8467\n",
      "RandomForestClassifier-100-None-5-2; 0.8487\n",
      "RandomForestClassifier-100-None-5-4; 0.8534\n",
      "RandomForestClassifier-100-None-10-1; 0.8442\n",
      "RandomForestClassifier-100-None-10-2; 0.8480\n",
      "RandomForestClassifier-100-None-10-4; 0.8598\n",
      "RandomForestClassifier-100-10-2-1; 0.8456\n",
      "RandomForestClassifier-100-10-2-2; 0.8501\n",
      "RandomForestClassifier-100-10-2-4; 0.8610\n",
      "RandomForestClassifier-100-10-5-1; 0.8540\n",
      "RandomForestClassifier-100-10-5-2; 0.8496\n",
      "RandomForestClassifier-100-10-5-4; 0.8543\n",
      "RandomForestClassifier-100-10-10-1; 0.8477\n",
      "RandomForestClassifier-100-10-10-2; 0.8589\n",
      "RandomForestClassifier-100-10-10-4; 0.8498\n",
      "RandomForestClassifier-100-20-2-1; 0.8487\n",
      "RandomForestClassifier-100-20-2-2; 0.8584\n",
      "RandomForestClassifier-100-20-2-4; 0.8554\n",
      "RandomForestClassifier-100-20-5-1; 0.8509\n",
      "RandomForestClassifier-100-20-5-2; 0.8557\n",
      "RandomForestClassifier-100-20-5-4; 0.8563\n",
      "RandomForestClassifier-100-20-10-1; 0.8484\n",
      "RandomForestClassifier-100-20-10-2; 0.8600\n",
      "RandomForestClassifier-100-20-10-4; 0.8532\n",
      "LogisticRegression-0.001-l1-liblinear; 0.5000\n",
      "LogisticRegression-0.001-l1-saga; 0.5000\n",
      "LogisticRegression-0.001-l2-liblinear; 0.8843\n",
      "LogisticRegression-0.001-l2-saga; 0.8863\n",
      "LogisticRegression-0.01-l1-liblinear; 0.8203\n",
      "LogisticRegression-0.01-l1-saga; 0.8201\n",
      "LogisticRegression-0.01-l2-liblinear; 0.8745\n",
      "LogisticRegression-0.01-l2-saga; 0.8772\n",
      "LogisticRegression-0.1-l1-liblinear; 0.8781\n",
      "LogisticRegression-0.1-l1-saga; 0.8821\n",
      "LogisticRegression-0.1-l2-liblinear; 0.8438\n",
      "LogisticRegression-0.1-l2-saga; 0.8631\n",
      "LogisticRegression-1-l1-liblinear; 0.8278\n",
      "LogisticRegression-1-l1-saga; 0.8664\n",
      "LogisticRegression-1-l2-liblinear; 0.8145\n",
      "LogisticRegression-1-l2-saga; 0.8612\n",
      "LogisticRegression-10-l1-liblinear; 0.7962\n",
      "LogisticRegression-10-l1-saga; 0.8615\n",
      "LogisticRegression-10-l2-liblinear; 0.7972\n",
      "LogisticRegression-10-l2-saga; 0.8606\n",
      "LogisticRegression-100-l1-liblinear; 0.7789\n",
      "LogisticRegression-100-l1-saga; 0.8607\n",
      "LogisticRegression-100-l2-liblinear; 0.7883\n",
      "LogisticRegression-100-l2-saga; 0.8607\n",
      "MLPClassifier-(50,)-relu-0.0001-200; 0.8147\n",
      "MLPClassifier-(50,)-relu-0.0001-500; 0.8187\n",
      "MLPClassifier-(50,)-relu-0.0001-1000; 0.8275\n",
      "MLPClassifier-(50,)-relu-0.001-200; 0.8118\n",
      "MLPClassifier-(50,)-relu-0.001-500; 0.8212\n",
      "MLPClassifier-(50,)-relu-0.001-1000; 0.8209\n",
      "MLPClassifier-(50,)-relu-0.01-200; 0.8240\n",
      "MLPClassifier-(50,)-relu-0.01-500; 0.8252\n",
      "MLPClassifier-(50,)-relu-0.01-1000; 0.8267\n",
      "MLPClassifier-(50,)-tanh-0.0001-200; 0.8194\n",
      "MLPClassifier-(50,)-tanh-0.0001-500; 0.8282\n",
      "MLPClassifier-(50,)-tanh-0.0001-1000; 0.8186\n",
      "MLPClassifier-(50,)-tanh-0.001-200; 0.8190\n",
      "MLPClassifier-(50,)-tanh-0.001-500; 0.8168\n",
      "MLPClassifier-(50,)-tanh-0.001-1000; 0.8212\n",
      "MLPClassifier-(50,)-tanh-0.01-200; 0.8293\n",
      "MLPClassifier-(50,)-tanh-0.01-500; 0.8262\n",
      "MLPClassifier-(50,)-tanh-0.01-1000; 0.8254\n",
      "MLPClassifier-(100,)-relu-0.0001-200; 0.8226\n",
      "MLPClassifier-(100,)-relu-0.0001-500; 0.8356\n",
      "MLPClassifier-(100,)-relu-0.0001-1000; 0.8285\n",
      "MLPClassifier-(100,)-relu-0.001-200; 0.8257\n",
      "MLPClassifier-(100,)-relu-0.001-500; 0.8326\n",
      "MLPClassifier-(100,)-relu-0.001-1000; 0.8210\n",
      "MLPClassifier-(100,)-relu-0.01-200; 0.8159\n",
      "MLPClassifier-(100,)-relu-0.01-500; 0.8190\n",
      "MLPClassifier-(100,)-relu-0.01-1000; 0.8265\n",
      "MLPClassifier-(100,)-tanh-0.0001-200; 0.8236\n",
      "MLPClassifier-(100,)-tanh-0.0001-500; 0.8225\n",
      "MLPClassifier-(100,)-tanh-0.0001-1000; 0.8285\n",
      "MLPClassifier-(100,)-tanh-0.001-200; 0.8184\n",
      "MLPClassifier-(100,)-tanh-0.001-500; 0.8189\n",
      "MLPClassifier-(100,)-tanh-0.001-1000; 0.8198\n",
      "MLPClassifier-(100,)-tanh-0.01-200; 0.8247\n",
      "MLPClassifier-(100,)-tanh-0.01-500; 0.8300\n",
      "MLPClassifier-(100,)-tanh-0.01-1000; 0.8351\n",
      "MLPClassifier-(50, 50)-relu-0.0001-200; 0.8312\n",
      "MLPClassifier-(50, 50)-relu-0.0001-500; 0.8188\n",
      "MLPClassifier-(50, 50)-relu-0.0001-1000; 0.8156\n",
      "MLPClassifier-(50, 50)-relu-0.001-200; 0.8267\n",
      "MLPClassifier-(50, 50)-relu-0.001-500; 0.8216\n",
      "MLPClassifier-(50, 50)-relu-0.001-1000; 0.8264\n",
      "MLPClassifier-(50, 50)-relu-0.01-200; 0.8229\n",
      "MLPClassifier-(50, 50)-relu-0.01-500; 0.8193\n",
      "MLPClassifier-(50, 50)-relu-0.01-1000; 0.8092\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-200; 0.8173\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-500; 0.8200\n",
      "MLPClassifier-(50, 50)-tanh-0.0001-1000; 0.8265\n",
      "MLPClassifier-(50, 50)-tanh-0.001-200; 0.8228\n",
      "MLPClassifier-(50, 50)-tanh-0.001-500; 0.8242\n",
      "MLPClassifier-(50, 50)-tanh-0.001-1000; 0.8393\n",
      "MLPClassifier-(50, 50)-tanh-0.01-200; 0.8314\n",
      "MLPClassifier-(50, 50)-tanh-0.01-500; 0.8304\n",
      "MLPClassifier-(50, 50)-tanh-0.01-1000; 0.8302\n",
      "GradientBoostingClassifier-10-0.1; 0.8358\n",
      "GradientBoostingClassifier-10-0.5; 0.8340\n",
      "GradientBoostingClassifier-10-1.0; 0.7931\n",
      "GradientBoostingClassifier-10-2.0; 0.7134\n",
      "GradientBoostingClassifier-100-0.1; 0.8688\n",
      "GradientBoostingClassifier-100-0.5; 0.8605\n",
      "GradientBoostingClassifier-100-1.0; 0.8351\n",
      "GradientBoostingClassifier-100-2.0; 0.7083\n",
      "GradientBoostingClassifier-200-0.1; 0.8623\n",
      "GradientBoostingClassifier-200-0.5; 0.8591\n",
      "GradientBoostingClassifier-200-1.0; 0.8323\n",
      "GradientBoostingClassifier-200-2.0; 0.7039\n"
     ]
    }
   ],
   "source": [
    "run_grid = {\n",
    "        'regression': [False],\n",
    "        'fingerprint': [False, True],\n",
    "        'pca': [False, True]\n",
    "    }\n",
    "\n",
    "run_param_combinations = list(product(*run_grid.values()))\n",
    "\n",
    "for combination in run_param_combinations:\n",
    "    r, f, p = combination\n",
    "    run_configured(r, f, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
